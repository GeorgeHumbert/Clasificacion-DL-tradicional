{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e4afc77-a2d4-4d44-8760-68893fca1993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hopsworks\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os  # Agregar esta línea para importar os\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "789184dd-cd7f-4b44-86bb-7ba8f584f764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection closed.\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1047704\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (15.30s) \n"
     ]
    }
   ],
   "source": [
    "# Conectar con Hopsworks\n",
    "project = hopsworks.login(api_key_value=os.getenv(\"HOPSWORKS_API_KEY\"))\n",
    "fs = project.get_feature_store()\n",
    "\n",
    "# Cargar el dataset desde el Feature Store\n",
    "imdb_fg = fs.get_feature_group(name=\"imdb_reviews_dl\", version=1)\n",
    "df = imdb_fg.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3e0bb7e-59d9-4c81-ba55-f3db3b24f657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 200, 64)           320000    \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               (None, 200, 64)           33024     \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 32)                12416     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 366529 (1.40 MB)\n",
      "Trainable params: 366529 (1.40 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "500/500 [==============================] - 62s 119ms/step - loss: 0.3868 - accuracy: 0.8226 - val_loss: 0.2966 - val_accuracy: 0.8795\n",
      "Epoch 2/5\n",
      "500/500 [==============================] - 62s 123ms/step - loss: 0.2623 - accuracy: 0.8939 - val_loss: 0.3012 - val_accuracy: 0.8779\n",
      "Epoch 3/5\n",
      "500/500 [==============================] - 62s 123ms/step - loss: 0.2377 - accuracy: 0.9048 - val_loss: 0.2979 - val_accuracy: 0.8816\n",
      "Epoch 4/5\n",
      "500/500 [==============================] - 62s 124ms/step - loss: 0.1910 - accuracy: 0.9272 - val_loss: 0.3100 - val_accuracy: 0.8755\n",
      "Epoch 5/5\n",
      "500/500 [==============================] - 62s 124ms/step - loss: 0.1813 - accuracy: 0.9300 - val_loss: 0.3371 - val_accuracy: 0.8675\n"
     ]
    }
   ],
   "source": [
    "# Separar características (X) y etiquetas (y)\n",
    "X = df.drop(columns=['sentiment', 'id'])  # 'sentiment' es la etiqueta y 'id' es el índice\n",
    "y = df['sentiment']\n",
    "\n",
    "# Dividir el dataset en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definir el modelo de Deep Learning\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=5000, output_dim=64, input_length=X_train.shape[1]),  # Capa de embedding\n",
    "    tf.keras.layers.LSTM(64, return_sequences=True),  # Primera capa LSTM\n",
    "    tf.keras.layers.LSTM(32),  # Segunda capa LSTM\n",
    "    tf.keras.layers.Dense(32, activation='relu'),  # Capa densa\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')  # Capa de salida\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Resumen del modelo\n",
    "model.summary()\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test), batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5891c8c4-5b35-4b25-8316-1fcb98274d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 - 7s - loss: 0.3371 - accuracy: 0.8675 - 7s/epoch - 29ms/step\n",
      "Test Accuracy: 0.8675\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo en el conjunto de prueba\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bc77261-604b-46a6-a4dd-cf711ad5d3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-21 03:10:17,741 WARNING: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Guardar el modelo entrenado\n",
    "model.save(\"modelo_imdb_dl.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "529e8203-3e5a-48cf-bc89-dbe84877b6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word index length: 124253\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import joblib\n",
    "\n",
    "# Asegúrate de que el DataFrame tenga una columna 'review' con los textos\n",
    "df = pd.read_csv(\"data/IMDB Dataset.csv\")  # Ajusta el nombre del archivo y la ruta\n",
    "\n",
    "# Parámetros del tokenizador\n",
    "vocab_size = 33000\n",
    "oov_tok = \"<OOV>\"\n",
    "max_length = 200  # Ajusta la longitud máxima según tu modelo previo\n",
    "\n",
    "# Inicializa el tokenizador\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "\n",
    "# Entrenar el tokenizador en los textos de las reseñas\n",
    "tokenizer.fit_on_texts(df['review'])\n",
    "\n",
    "# Convertir los textos a secuencias numéricas\n",
    "sequences = tokenizer.texts_to_sequences(df['review'])\n",
    "\n",
    "# Rellenar (pad) las secuencias para que todas tengan la misma longitud\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length)\n",
    "\n",
    "# Guardar el tokenizador para usarlo en la API y la notebook 3\n",
    "joblib.dump(tokenizer, \"tokenizer.pkl\")\n",
    "\n",
    "# Verificar el word_index del tokenizador\n",
    "print(f\"Word index length: {len(tokenizer.word_index)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a2f5436c-0ce2-4e66-9b34-9ed6cdb3be36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "625/625 [==============================] - 82s 128ms/step - loss: 0.3556 - accuracy: 0.8386 - val_loss: 0.2738 - val_accuracy: 0.8856\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 80s 129ms/step - loss: 0.1795 - accuracy: 0.9322 - val_loss: 0.2906 - val_accuracy: 0.8836\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 78s 126ms/step - loss: 0.1092 - accuracy: 0.9619 - val_loss: 0.3461 - val_accuracy: 0.8813\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 79s 126ms/step - loss: 0.0733 - accuracy: 0.9748 - val_loss: 0.3709 - val_accuracy: 0.8740\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 79s 127ms/step - loss: 0.0570 - accuracy: 0.9814 - val_loss: 0.4702 - val_accuracy: 0.8733\n",
      "313/313 [==============================] - 8s 27ms/step - loss: 0.4702 - accuracy: 0.8733\n",
      "Validation Accuracy: 0.8733000159263611\n",
      "2024-09-21 04:09:49,904 WARNING: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Definir el modelo LSTM\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=64, input_length=max_length),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    LSTM(32),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Definir etiquetas (sentimientos) y preparar los datos de entrenamiento y validación\n",
    "y = df['sentiment'].apply(lambda x: 1 if x == \"positive\" else 0).values\n",
    "\n",
    "# Separar en conjunto de entrenamiento y validación\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(padded_sequences, y, test_size=0.2)\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(X_train, y_train, epochs=5, validation_data=(X_val, y_val), batch_size=64)\n",
    "\n",
    "# Evaluar el modelo en el conjunto de validación\n",
    "val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "model.save(\"modelo_imdb_dl.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b87bcee-d559-49d3-8a07-54c9385578d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
